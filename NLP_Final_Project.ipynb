{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Final Project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "g9MgCBgsOdxl",
        "9cSJy8gpRy_P",
        "Pup75Rwo5Rsh"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9MgCBgsOdxl"
      },
      "source": [
        "# Data Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soNluLzp2-2g"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import time"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQL35COTds20"
      },
      "source": [
        "Reading Data into Lists"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFrHxwBJbRze",
        "outputId": "20019cb9-1c07-4c08-e781-0247a32a55ce"
      },
      "source": [
        "lines = open(\"movie_lines.txt\", encoding = \"UTF-8\", errors = \"ignore\").read().split(\"\\n\")\n",
        "lines[:5]"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!',\n",
              " 'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!',\n",
              " 'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.',\n",
              " 'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?',\n",
              " \"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSBjkxvqbiT_",
        "outputId": "99395e25-7f07-4cc9-84ef-02473f699fd0"
      },
      "source": [
        "conversations = open('movie_conversations.txt', encoding = 'UTF-8', errors = 'ignore').read().split('\\n')\n",
        "conversations[:5]"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\",\n",
              " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\",\n",
              " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\",\n",
              " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L205', 'L206']\",\n",
              " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dS2IUlfd0qE"
      },
      "source": [
        "Reading Lists into Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSDtbW-NdKeE"
      },
      "source": [
        "id2line = {}\n",
        "for line in lines:\n",
        "    _line = line.split(' +++$+++ ')\n",
        "    if len(_line) == 5:\n",
        "      id2line[_line[0]] = _line[4]"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4vETHGoeG4m",
        "outputId": "4fd784d7-53b6-49f9-b377-ee5608b2870f"
      },
      "source": [
        "conversations_ids = []\n",
        "for conversation in conversations[:-1]:\n",
        "    _conversation = conversation.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \"\").replace(\" \", \"\")\n",
        "    conversations_ids.append(_conversation.split(','))\n",
        "conversations_ids[:2]"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['L194', 'L195', 'L196', 'L197'], ['L198', 'L199']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oCceDt-0AYC"
      },
      "source": [
        "Questions and Answers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OU_Ma5vklOL"
      },
      "source": [
        "questions = []\n",
        "answers = []\n",
        "\n",
        "for conversation in conversations_ids:\n",
        "    for i in range(len(conversation) - 1):\n",
        "        questions.append(id2line[conversation[i]])\n",
        "        answers.append(id2line[conversation[i+1]])"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5Z7vmwQ0RqC",
        "outputId": "ac73e418-6636-4d84-f675-c49bada6cb56"
      },
      "source": [
        "questions[:2]"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.',\n",
              " \"Well, I thought we'd start with pronunciation, if that's okay with you.\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sIfA1sK0TkC",
        "outputId": "103dabbd-5500-4f1b-9f44-52456c48de1c"
      },
      "source": [
        "answers[:2]"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Well, I thought we'd start with pronunciation, if that's okay with you.\",\n",
              " 'Not the hacking and gagging and spitting part.  Please.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1SwEpBl2t6D"
      },
      "source": [
        "Cleaning the Questions and Answers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjHgGZCG0bkD"
      },
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"what is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
        "    return text"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdJpPM1H20lN",
        "outputId": "92852e54-a79f-4e4d-8cfe-b7c7e7af32c0"
      },
      "source": [
        "clean_questions = []\n",
        "for question in questions:\n",
        "    clean_questions.append(clean_text(question))\n",
        "clean_questions[:2]"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['can we make this quick  roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad  again',\n",
              " 'well i thought we would start with pronunciation if that is okay with you']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1YgYVL658my",
        "outputId": "89e3fa4d-8a9b-409d-df73-145f4577655d"
      },
      "source": [
        "clean_answers = []\n",
        "for answer in answers:\n",
        "    clean_answers.append(clean_text(answer))\n",
        "clean_answers[:2]"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['well i thought we would start with pronunciation if that is okay with you',\n",
              " 'not the hacking and gagging and spitting part  please']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjteQJe678pt"
      },
      "source": [
        "Filtering out Questions and Answers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5cyG2hr6_tX",
        "outputId": "b037642e-7e2e-4490-d3c3-f289b44144d1"
      },
      "source": [
        "short_questions = []\n",
        "short_answers = []\n",
        "i = 0\n",
        "for question in clean_questions:\n",
        "    if 2 <= len(question.split()) <= 25:\n",
        "        short_questions.append(question)\n",
        "        short_answers.append(clean_answers[i])\n",
        "    i += 1\n",
        "short_questions[:2]"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['can we make this quick  roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad  again',\n",
              " 'well i thought we would start with pronunciation if that is okay with you']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pYtRgq-9GI7",
        "outputId": "a5e62a73-89e7-4fc9-cbbb-3ff2afc07760"
      },
      "source": [
        "short_answers[:2]"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['well i thought we would start with pronunciation if that is okay with you',\n",
              " 'not the hacking and gagging and spitting part  please']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6vg_Sr19CFN"
      },
      "source": [
        "clean_questions = []\n",
        "clean_answers = []\n",
        "i = 0\n",
        "for answer in short_answers:\n",
        "    if 2 <= len(answer.split()) <= 25:\n",
        "        clean_answers.append(answer)\n",
        "        clean_questions.append(short_questions[i])\n",
        "    i += 1"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldjKM-CS9Cvx",
        "outputId": "5e158ec2-1a19-49eb-9ec5-088e2378c321"
      },
      "source": [
        "clean_answers[:2]"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['well i thought we would start with pronunciation if that is okay with you',\n",
              " 'not the hacking and gagging and spitting part  please']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNXkg0eh9T-O",
        "outputId": "ba056f4c-13c0-48ed-e573-c18ee248910d"
      },
      "source": [
        "clean_questions[:2]"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['can we make this quick  roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad  again',\n",
              " 'well i thought we would start with pronunciation if that is okay with you']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vEGU-bLGbj-"
      },
      "source": [
        "Word and it's count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx8kYZxQ9Vbj"
      },
      "source": [
        "word2count = {}\n",
        "for question in clean_questions:\n",
        "    for word in question.split():\n",
        "        if word not in word2count:\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1\n",
        "for answer in clean_answers:\n",
        "    for word in answer.split():\n",
        "        if word not in word2count:\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1\n",
        "#word2count"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtNZl6MqKinB"
      },
      "source": [
        "Mapping Word to Integer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZvWJdCJGpM5"
      },
      "source": [
        "threshold_answers = 15\n",
        "answerswords2int = {}\n",
        "word_number = 0\n",
        "for word, count in word2count.items():\n",
        "    if count >= threshold_answers:\n",
        "        answerswords2int[word] = word_number\n",
        "        word_number += 1\n",
        "threshold_questions = 15\n",
        "questionswords2int = {}\n",
        "word_number = 0\n",
        "for word, count in word2count.items():\n",
        "    if count >= threshold_questions:\n",
        "        questionswords2int[word] = word_number\n",
        "        word_number += 1\n",
        "#questionswords2int"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II3Ry5mmCAHq"
      },
      "source": [
        "Adding Tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEw783pP9SLq"
      },
      "source": [
        "tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\n",
        "\n",
        "for token in tokens:\n",
        "    questionswords2int[token] = len(questionswords2int) +1\n",
        "\n",
        "for token in tokens:\n",
        "    answerswords2int[token] = len(answerswords2int) +1"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uj2bn-CACORM",
        "outputId": "5703cb68-59d3-4c58-e207-6b2f0bbbd154"
      },
      "source": [
        "len(questionswords2int)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6998"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8KWLZiqCRVr",
        "outputId": "1cf95c1f-f574-412d-ca76-b779fe8c052c"
      },
      "source": [
        "len(answerswords2int)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6998"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BC1_hFKlJlfj"
      },
      "source": [
        "#answerswords2int"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaqmYZJwD565"
      },
      "source": [
        "Inverse Dictionary and adding 'EOS' token"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-CMxCYJCVuv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17b9fb54-8912-4f2c-a15b-924f2541f954"
      },
      "source": [
        "answersints2word = {w_i: w for w, w_i in answerswords2int.items()}\n",
        "answersints2word"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'can',\n",
              " 1: 'we',\n",
              " 2: 'make',\n",
              " 3: 'this',\n",
              " 4: 'quick',\n",
              " 5: 'and',\n",
              " 6: 'andrew',\n",
              " 7: 'are',\n",
              " 8: 'having',\n",
              " 9: 'an',\n",
              " 10: 'incredibly',\n",
              " 11: 'public',\n",
              " 12: 'break',\n",
              " 13: 'up',\n",
              " 14: 'on',\n",
              " 15: 'the',\n",
              " 16: 'again',\n",
              " 17: 'well',\n",
              " 18: 'i',\n",
              " 19: 'thought',\n",
              " 20: 'would',\n",
              " 21: 'start',\n",
              " 22: 'with',\n",
              " 23: 'if',\n",
              " 24: 'that',\n",
              " 25: 'is',\n",
              " 26: 'okay',\n",
              " 27: 'you',\n",
              " 28: 'not',\n",
              " 29: 'part',\n",
              " 30: 'please',\n",
              " 31: 'asking',\n",
              " 32: 'me',\n",
              " 33: 'out',\n",
              " 34: 'so',\n",
              " 35: 'cute',\n",
              " 36: 'what',\n",
              " 37: 'your',\n",
              " 38: 'name',\n",
              " 39: 'thing',\n",
              " 40: 'cameron',\n",
              " 41: 'am',\n",
              " 42: 'at',\n",
              " 43: 'mercy',\n",
              " 44: 'of',\n",
              " 45: 'a',\n",
              " 46: 'particularly',\n",
              " 47: 'breed',\n",
              " 48: 'loser',\n",
              " 49: 'my',\n",
              " 50: 'sister',\n",
              " 51: 'ca',\n",
              " 52: 'date',\n",
              " 53: 'until',\n",
              " 54: 'she',\n",
              " 55: 'does',\n",
              " 56: 'mystery',\n",
              " 57: 'used',\n",
              " 58: 'to',\n",
              " 59: 'be',\n",
              " 60: 'really',\n",
              " 61: 'popular',\n",
              " 62: 'when',\n",
              " 63: 'started',\n",
              " 64: 'high',\n",
              " 65: 'school',\n",
              " 66: 'then',\n",
              " 67: 'it',\n",
              " 68: 'was',\n",
              " 69: 'just',\n",
              " 70: 'like',\n",
              " 71: 'got',\n",
              " 72: 'sick',\n",
              " 73: 'or',\n",
              " 74: 'something',\n",
              " 75: 'gosh',\n",
              " 76: 'only',\n",
              " 77: 'could',\n",
              " 78: 'find',\n",
              " 79: 'kat',\n",
              " 80: 'boyfriend',\n",
              " 81: 'ma',\n",
              " 82: 'head',\n",
              " 83: 'because',\n",
              " 84: \"it's\",\n",
              " 85: 'such',\n",
              " 86: 'nice',\n",
              " 87: 'one',\n",
              " 88: 'how',\n",
              " 89: 'our',\n",
              " 90: 'little',\n",
              " 91: 'plan',\n",
              " 92: 'mind',\n",
              " 93: 'have',\n",
              " 94: 'word',\n",
              " 95: 'as',\n",
              " 96: 'gentleman',\n",
              " 97: 'do',\n",
              " 98: 'get',\n",
              " 99: 'hair',\n",
              " 100: 'look',\n",
              " 101: 'sure',\n",
              " 102: 'wanna',\n",
              " 103: 'go',\n",
              " 104: 'but',\n",
              " 105: 'unless',\n",
              " 106: 'goes',\n",
              " 107: 'lesbian',\n",
              " 108: 'no',\n",
              " 109: 'found',\n",
              " 110: 'picture',\n",
              " 111: 'in',\n",
              " 112: 'her',\n",
              " 113: 'pretty',\n",
              " 114: 'kind',\n",
              " 115: 'guy',\n",
              " 116: 'likes',\n",
              " 117: 'ones',\n",
              " 118: 'know',\n",
              " 119: 'looked',\n",
              " 120: 'for',\n",
              " 121: 'back',\n",
              " 122: 'party',\n",
              " 123: 'always',\n",
              " 124: 'seemed',\n",
              " 125: 'occupied',\n",
              " 126: 'listen',\n",
              " 127: 'crap',\n",
              " 128: 'blonde',\n",
              " 129: 'boring',\n",
              " 130: 'myself',\n",
              " 131: 'figured',\n",
              " 132: 'good',\n",
              " 133: 'stuff',\n",
              " 134: 'eventually',\n",
              " 135: 'real',\n",
              " 136: 'they',\n",
              " 137: 'he',\n",
              " 138: 'dry',\n",
              " 139: 'home',\n",
              " 140: 'twenty',\n",
              " 141: 'minutes',\n",
              " 142: 'think',\n",
              " 143: \"'\",\n",
              " 144: 're',\n",
              " 145: 'prom',\n",
              " 146: 'queen',\n",
              " 147: 'harry',\n",
              " 148: 'hey',\n",
              " 149: 'sweet',\n",
              " 150: 'cheeks',\n",
              " 151: 'hi',\n",
              " 152: 'joey',\n",
              " 153: 'want',\n",
              " 154: 'talk',\n",
              " 155: 'about',\n",
              " 156: 'where',\n",
              " 157: 'been',\n",
              " 158: 'potential',\n",
              " 159: 'way',\n",
              " 160: 'oh',\n",
              " 161: 'god',\n",
              " 162: 'mean',\n",
              " 163: 'becoming',\n",
              " 164: 'normal',\n",
              " 165: 'ruining',\n",
              " 166: 'wo',\n",
              " 167: 'forget',\n",
              " 168: 'night',\n",
              " 169: 'completely',\n",
              " 170: 'supposed',\n",
              " 171: 'even',\n",
              " 172: 'means',\n",
              " 173: 'bianca',\n",
              " 174: 'need',\n",
              " 175: 'tell',\n",
              " 176: 'hate',\n",
              " 177: 'sit',\n",
              " 178: 'susie',\n",
              " 179: 'care',\n",
              " 180: 'total',\n",
              " 181: 'babe',\n",
              " 182: 'now',\n",
              " 183: 'different',\n",
              " 184: 'story',\n",
              " 185: 'said',\n",
              " 186: 'everyone',\n",
              " 187: 'doing',\n",
              " 188: 'did',\n",
              " 189: 'why',\n",
              " 190: 'stupid',\n",
              " 191: 'enough',\n",
              " 192: 'repeat',\n",
              " 193: 'mistakes',\n",
              " 194: 'guess',\n",
              " 195: 'protecting',\n",
              " 196: 'him',\n",
              " 197: 'keep',\n",
              " 198: 'locked',\n",
              " 199: 'away',\n",
              " 200: 'dark',\n",
              " 201: 'experience',\n",
              " 202: 'anything',\n",
              " 203: 'all',\n",
              " 204: 'experiences',\n",
              " 205: 'trust',\n",
              " 206: 'people',\n",
              " 207: 'beautiful',\n",
              " 208: 'last',\n",
              " 209: 'let',\n",
              " 210: 'set',\n",
              " 211: 'wanted',\n",
              " 212: 'damage',\n",
              " 213: 'send',\n",
              " 214: 'therapy',\n",
              " 215: 'forever',\n",
              " 216: 'woman',\n",
              " 217: 'complete',\n",
              " 218: 'upset',\n",
              " 219: 'daddy',\n",
              " 220: \"there's\",\n",
              " 221: 'boy',\n",
              " 222: 'might',\n",
              " 223: 'ask',\n",
              " 224: 'dating',\n",
              " 225: 'starts',\n",
              " 226: 'end',\n",
              " 227: 'discussion',\n",
              " 228: 'never',\n",
              " 229: 'neither',\n",
              " 230: 'will',\n",
              " 231: 'sleep',\n",
              " 232: 'going',\n",
              " 233: 'must',\n",
              " 234: 'were',\n",
              " 235: 'small',\n",
              " 236: 'study',\n",
              " 237: 'group',\n",
              " 238: 'friends',\n",
              " 239: 'otherwise',\n",
              " 240: 'known',\n",
              " 241: 'expect',\n",
              " 242: 'there',\n",
              " 243: 'starting',\n",
              " 244: 'wear',\n",
              " 245: 'belly',\n",
              " 246: 'before',\n",
              " 247: 'discuss',\n",
              " 248: 'tomorrow',\n",
              " 249: 'has',\n",
              " 250: 'hot',\n",
              " 251: 'rod',\n",
              " 252: 'right',\n",
              " 253: 's',\n",
              " 254: 'who',\n",
              " 255: 'bend',\n",
              " 256: 'rules',\n",
              " 257: 'whatever',\n",
              " 258: 'missing',\n",
              " 259: 'pleasure',\n",
              " 260: 'fan',\n",
              " 261: 'too',\n",
              " 262: 'ring',\n",
              " 263: 'girls',\n",
              " 264: 'tall',\n",
              " 265: 'decent',\n",
              " 266: 'body',\n",
              " 267: 'other',\n",
              " 268: 'kinda',\n",
              " 269: 'short',\n",
              " 270: 'new',\n",
              " 271: 'which',\n",
              " 272: 'from',\n",
              " 273: 'north',\n",
              " 274: 'actually',\n",
              " 275: 'kidding',\n",
              " 276: 'live',\n",
              " 277: 'yeah',\n",
              " 278: 'couple',\n",
              " 279: 'by',\n",
              " 280: 'cows',\n",
              " 281: 'though',\n",
              " 282: 'many',\n",
              " 283: 'here',\n",
              " 284: 'girl',\n",
              " 285: 'burn',\n",
              " 286: 'guys',\n",
              " 287: 'their',\n",
              " 288: 'mothers',\n",
              " 289: 'liked',\n",
              " 290: 'them',\n",
              " 291: 'gene',\n",
              " 292: 'pool',\n",
              " 293: 'rarely',\n",
              " 294: 'french',\n",
              " 295: \"mom's\",\n",
              " 296: 'canada',\n",
              " 297: 'signed',\n",
              " 298: 'chance',\n",
              " 299: 'minor',\n",
              " 300: 'encounter',\n",
              " 301: 'teach',\n",
              " 302: 'charm',\n",
              " 303: 'falls',\n",
              " 304: 'love',\n",
              " 305: 'makes',\n",
              " 306: 'seems',\n",
              " 307: 'danger',\n",
              " 308: 'criminal',\n",
              " 309: 'heard',\n",
              " 310: 'lit',\n",
              " 311: 'state',\n",
              " 312: 'fire',\n",
              " 313: 'serious',\n",
              " 314: 'man',\n",
              " 315: 'whacked',\n",
              " 316: 'sold',\n",
              " 317: 'his',\n",
              " 318: 'own',\n",
              " 319: 'liver',\n",
              " 320: 'black',\n",
              " 321: 'market',\n",
              " 322: 'buy',\n",
              " 323: 'reputation',\n",
              " 324: 'say',\n",
              " 325: 'strictly',\n",
              " 326: 'side',\n",
              " 327: 'hated',\n",
              " 328: 'those',\n",
              " 329: 'golden',\n",
              " 330: 'opportunity',\n",
              " 331: 'patrick',\n",
              " 332: 'case',\n",
              " 333: 'blow',\n",
              " 334: 'bent',\n",
              " 335: 'number',\n",
              " 336: 'hates',\n",
              " 337: 'lung',\n",
              " 338: 'cancer',\n",
              " 339: 'issue',\n",
              " 340: 'favorite',\n",
              " 341: 'uncle',\n",
              " 342: 'ears',\n",
              " 343: 'told',\n",
              " 344: 'already',\n",
              " 345: 'extremely',\n",
              " 346: 'unfortunate',\n",
              " 347: 'hell',\n",
              " 348: 'while',\n",
              " 349: 'talking',\n",
              " 350: 'making',\n",
              " 351: 'progress',\n",
              " 352: 'humiliated',\n",
              " 353: 'sacrifice',\n",
              " 354: 'yourself',\n",
              " 355: 'dignity',\n",
              " 356: 'score',\n",
              " 357: 'non',\n",
              " 358: 'type',\n",
              " 359: 'playing',\n",
              " 360: 'busy',\n",
              " 361: 'thousand',\n",
              " 362: 'direct',\n",
              " 363: 'quote',\n",
              " 364: \"makin'\",\n",
              " 365: 'any',\n",
              " 366: 'worst',\n",
              " 367: 'over',\n",
              " 368: 'reading',\n",
              " 369: 'noticed',\n",
              " 370: 'big',\n",
              " 371: 'spread',\n",
              " 372: 'tough',\n",
              " 373: 'leave',\n",
              " 374: 'alone',\n",
              " 375: 'two',\n",
              " 376: 'legs',\n",
              " 377: 'rack',\n",
              " 378: 'money',\n",
              " 379: 'take',\n",
              " 380: 'pick',\n",
              " 381: 'tab',\n",
              " 382: 'gonna',\n",
              " 383: 'pay',\n",
              " 384: 'some',\n",
              " 385: 'gets',\n",
              " 386: 'catch',\n",
              " 387: 'bucks',\n",
              " 388: 'shell',\n",
              " 389: 'fifty',\n",
              " 390: 'results',\n",
              " 391: 'watching',\n",
              " 392: 'bitch',\n",
              " 393: 'trash',\n",
              " 394: 'car',\n",
              " 395: 'count',\n",
              " 396: 'hundred',\n",
              " 397: 'time',\n",
              " 398: 'flowers',\n",
              " 399: 'another',\n",
              " 400: 'lost',\n",
              " 401: 'nope',\n",
              " 402: 'came',\n",
              " 403: 'chat',\n",
              " 404: 'run',\n",
              " 405: 'idea',\n",
              " 406: 'see',\n",
              " 407: 'interested',\n",
              " 408: 'insane',\n",
              " 409: 'conversation',\n",
              " 410: 'purpose',\n",
              " 411: 'hear',\n",
              " 412: 'uh',\n",
              " 413: 'old',\n",
              " 414: 'better',\n",
              " 415: 'fuck',\n",
              " 416: 'heavily',\n",
              " 417: \"who's\",\n",
              " 418: 'random',\n",
              " 419: 'pat',\n",
              " 420: 'gone',\n",
              " 421: 'year',\n",
              " 422: 'porn',\n",
              " 423: 'movies',\n",
              " 424: 'interesting',\n",
              " 425: 'eat',\n",
              " 426: 'starving',\n",
              " 427: 'very',\n",
              " 428: 'slow',\n",
              " 429: 'die',\n",
              " 430: 'foul',\n",
              " 431: 'william',\n",
              " 432: 'proven',\n",
              " 433: 'appreciate',\n",
              " 434: 'toward',\n",
              " 435: 'death',\n",
              " 436: 'matter',\n",
              " 437: 'work',\n",
              " 438: 'went',\n",
              " 439: 'officially',\n",
              " 440: 'opposed',\n",
              " 441: 'social',\n",
              " 442: 'activity',\n",
              " 443: 'choice',\n",
              " 444: 'done',\n",
              " 445: 'favor',\n",
              " 446: 'imagine',\n",
              " 447: 'commercial',\n",
              " 448: 'since',\n",
              " 449: 'dates',\n",
              " 450: 'sound',\n",
              " 451: 'betty',\n",
              " 452: 'pissed',\n",
              " 453: 'off',\n",
              " 454: 'archie',\n",
              " 455: 'taking',\n",
              " 456: 'veronica',\n",
              " 457: 'dress',\n",
              " 458: 'anyway',\n",
              " 459: 'looking',\n",
              " 460: 'wrong',\n",
              " 461: 'perspective',\n",
              " 462: 'statement',\n",
              " 463: 'asked',\n",
              " 464: 'meet',\n",
              " 465: 'ya',\n",
              " 466: \"doin'\",\n",
              " 467: 'sweating',\n",
              " 468: 'pig',\n",
              " 469: \"guy's\",\n",
              " 470: 'attention',\n",
              " 471: 'friday',\n",
              " 472: 'places',\n",
              " 473: 'warrant',\n",
              " 474: 'strong',\n",
              " 475: 'emotion',\n",
              " 476: 'spend',\n",
              " 477: 'dollar',\n",
              " 478: 'track',\n",
              " 479: 'come',\n",
              " 480: 'flat',\n",
              " 481: 'beer',\n",
              " 482: 'eyes',\n",
              " 483: 'hand',\n",
              " 484: 'ass',\n",
              " 485: 'following',\n",
              " 486: 'excuse',\n",
              " 487: 'these',\n",
              " 488: 'kill',\n",
              " 489: 'getting',\n",
              " 490: 'lie',\n",
              " 491: 'down',\n",
              " 492: 'awhile',\n",
              " 493: 'use',\n",
              " 494: 'words',\n",
              " 495: 'needs',\n",
              " 496: 'blind',\n",
              " 497: 'deal',\n",
              " 498: 'tequila',\n",
              " 499: 'seem',\n",
              " 500: 'don',\n",
              " 501: 't',\n",
              " 502: 'cool',\n",
              " 503: 'laid',\n",
              " 504: 'else',\n",
              " 505: 'above',\n",
              " 506: 'control',\n",
              " 507: 'should',\n",
              " 508: 'band',\n",
              " 509: 'father',\n",
              " 510: 'approve',\n",
              " 511: 'dad',\n",
              " 512: 'pain',\n",
              " 513: 'seen',\n",
              " 514: 'copy',\n",
              " 515: 'poetry',\n",
              " 516: 'someone',\n",
              " 517: 'still',\n",
              " 518: 'panties',\n",
              " 519: 'twist',\n",
              " 520: 'minute',\n",
              " 521: 'had',\n",
              " 522: 'effect',\n",
              " 523: 'whatsoever',\n",
              " 524: 'left',\n",
              " 525: 'afraid',\n",
              " 526: 'heights',\n",
              " 527: \"c'mon\",\n",
              " 528: 'bad',\n",
              " 529: 'put',\n",
              " 530: 'foot',\n",
              " 531: \"stayin'\",\n",
              " 532: 'family',\n",
              " 533: 'ridiculous',\n",
              " 534: 'win',\n",
              " 535: 'respect',\n",
              " 536: 'piss',\n",
              " 537: 'call',\n",
              " 538: 'soft',\n",
              " 539: 'knew',\n",
              " 540: 'disappointed',\n",
              " 541: 'change',\n",
              " 542: 'disappoint',\n",
              " 543: 'covered',\n",
              " 544: 'true',\n",
              " 545: 'knows',\n",
              " 546: 'anyone',\n",
              " 547: 'ever',\n",
              " 548: 'request',\n",
              " 549: 'command',\n",
              " 550: 'create',\n",
              " 551: 'drama',\n",
              " 552: 'rumor',\n",
              " 553: 'motive',\n",
              " 554: 'answer',\n",
              " 555: 'question',\n",
              " 556: 'convicted',\n",
              " 557: 'huh',\n",
              " 558: 'adorable',\n",
              " 559: 'wait',\n",
              " 560: 'paid',\n",
              " 561: 'person',\n",
              " 562: 'truly',\n",
              " 563: 'setup',\n",
              " 564: 'payment',\n",
              " 565: 'bonus',\n",
              " 566: 'sleeping',\n",
              " 567: 'bought',\n",
              " 568: 'besides',\n",
              " 569: 'extra',\n",
              " 570: 'cash',\n",
              " 571: 'asshole',\n",
              " 572: 'great',\n",
              " 573: 'burger',\n",
              " 574: 'burnt',\n",
              " 575: 'object',\n",
              " 576: 'torture',\n",
              " 577: 'decided',\n",
              " 578: 'u',\n",
              " 579: 'insurance',\n",
              " 580: 'cover',\n",
              " 581: 'pms',\n",
              " 582: 'sarah',\n",
              " 583: 'agree',\n",
              " 584: 'decisions',\n",
              " 585: 'eighteen',\n",
              " 586: 'fortyfive',\n",
              " 587: 'understand',\n",
              " 588: 'food',\n",
              " 589: 'hip',\n",
              " 590: 'dance',\n",
              " 591: 'fun',\n",
              " 592: 'parts',\n",
              " 593: 'beat',\n",
              " 594: 'ms',\n",
              " 595: 'maintain',\n",
              " 596: 'kicked',\n",
              " 597: 'himself',\n",
              " 598: 'balls',\n",
              " 599: 'merely',\n",
              " 600: 'feel',\n",
              " 601: 'pictures',\n",
              " 602: 'situation',\n",
              " 603: 'major',\n",
              " 604: 'jones',\n",
              " 605: 'speak',\n",
              " 606: 'pure',\n",
              " 607: 'than',\n",
              " 608: 'help',\n",
              " 609: 'wild',\n",
              " 610: 'beast',\n",
              " 611: 'telling',\n",
              " 612: 'prefer',\n",
              " 613: 'simply',\n",
              " 614: 'alternative',\n",
              " 615: 'law',\n",
              " 616: 'cozy',\n",
              " 617: 'thy',\n",
              " 618: 'force',\n",
              " 619: 'missed',\n",
              " 620: 'says',\n",
              " 621: 'exposed',\n",
              " 622: 'eating',\n",
              " 623: 'lunch',\n",
              " 624: 'week',\n",
              " 625: 'feeling',\n",
              " 626: 'touch',\n",
              " 627: 'flu',\n",
              " 628: 'driving',\n",
              " 629: 'microwave',\n",
              " 630: 'rather',\n",
              " 631: 'pirate',\n",
              " 632: 'british',\n",
              " 633: 'rear',\n",
              " 634: 'admiral',\n",
              " 635: 'kiss',\n",
              " 636: 'watch',\n",
              " 637: 'honey',\n",
              " 638: 'happened',\n",
              " 639: 'daughters',\n",
              " 640: 'absolutely',\n",
              " 641: 'heat',\n",
              " 642: 'las',\n",
              " 643: \"water's\",\n",
              " 644: 'barrels',\n",
              " 645: 'listening',\n",
              " 646: 'chicken',\n",
              " 647: 'ah',\n",
              " 648: 'harm',\n",
              " 649: \"devil's\",\n",
              " 650: 'child',\n",
              " 651: 'land',\n",
              " 652: 'three',\n",
              " 653: 'weeks',\n",
              " 654: 'ago',\n",
              " 655: 'near',\n",
              " 656: 'asia',\n",
              " 657: 'sailing',\n",
              " 658: 'west',\n",
              " 659: 'yes',\n",
              " 660: 'voyage',\n",
              " 661: 'more',\n",
              " 662: 'six',\n",
              " 663: 'seven',\n",
              " 664: 'unfortunately',\n",
              " 665: 'colon',\n",
              " 666: 'precisely',\n",
              " 667: 'familiar',\n",
              " 668: 'life',\n",
              " 669: 'others',\n",
              " 670: 'interests',\n",
              " 671: 'gold',\n",
              " 672: 'intended',\n",
              " 673: 'believe',\n",
              " 674: 'waited',\n",
              " 675: 'show',\n",
              " 676: 'world',\n",
              " 677: 'realize',\n",
              " 678: 'considered',\n",
              " 679: 'reason',\n",
              " 680: \"man's\",\n",
              " 681: 'proposition',\n",
              " 682: 'judgment',\n",
              " 683: 'ours',\n",
              " 684: 'easy',\n",
              " 685: 'rid',\n",
              " 686: 'sanchez',\n",
              " 687: 'tragedy',\n",
              " 688: 'waste',\n",
              " 689: 'years',\n",
              " 690: 'given',\n",
              " 691: 'much',\n",
              " 692: 'marry',\n",
              " 693: 'leaving',\n",
              " 694: 'argue',\n",
              " 695: 'perhaps',\n",
              " 696: 'meant',\n",
              " 697: 'swear',\n",
              " 698: 'usually',\n",
              " 699: 'taken',\n",
              " 700: 'took',\n",
              " 701: 'everything',\n",
              " 702: 'stay',\n",
              " 703: 'us',\n",
              " 704: 'soon',\n",
              " 705: 'islands',\n",
              " 706: 'forgive',\n",
              " 707: 'may',\n",
              " 708: 'whom',\n",
              " 709: 'de',\n",
              " 710: 'letters',\n",
              " 711: 'appointment',\n",
              " 712: 'far',\n",
              " 713: 'sea',\n",
              " 714: 'hope',\n",
              " 715: 'mainland',\n",
              " 716: 'exactly',\n",
              " 717: 'holy',\n",
              " 718: 'saints',\n",
              " 719: 'heaven',\n",
              " 720: 'explore',\n",
              " 721: 'asks',\n",
              " 722: 'visit',\n",
              " 723: 'address',\n",
              " 724: 'remember',\n",
              " 725: 'beginning',\n",
              " 726: 'yet',\n",
              " 727: 'remind',\n",
              " 728: 'position',\n",
              " 729: 'bargain',\n",
              " 730: 'ambitious',\n",
              " 731: 'excellency',\n",
              " 732: 'ambition',\n",
              " 733: 'virtue',\n",
              " 734: 'among',\n",
              " 735: 'fault',\n",
              " 736: 'rest',\n",
              " 737: 'defend',\n",
              " 738: 'lack',\n",
              " 739: 'contact',\n",
              " 740: 'administration',\n",
              " 741: 'judge',\n",
              " 742: 'dear',\n",
              " 743: 'special',\n",
              " 744: 'talent',\n",
              " 745: 'guards',\n",
              " 746: 'nothing',\n",
              " 747: 'cuba',\n",
              " 748: 'island',\n",
              " 749: 'first',\n",
              " 750: 'chief',\n",
              " 751: 'thank',\n",
              " 752: 'bring',\n",
              " 753: 'also',\n",
              " 754: 'medicine',\n",
              " 755: 'peace',\n",
              " 756: 'understands',\n",
              " 757: 'diego',\n",
              " 758: 'bright',\n",
              " 759: 'brothers',\n",
              " 760: 'raised',\n",
              " 761: 'together',\n",
              " 762: 'manage',\n",
              " 763: 'wish',\n",
              " 764: 'sail',\n",
              " 765: 'certain',\n",
              " 766: 'ocean',\n",
              " 767: 'ignorance',\n",
              " 768: 'leagues',\n",
              " 769: 'canary',\n",
              " 770: 'jew',\n",
              " 771: 'dead',\n",
              " 772: 'passion',\n",
              " 773: 'try',\n",
              " 774: 'cannot',\n",
              " 775: 'carried',\n",
              " 776: 'being',\n",
              " 777: 'eternity',\n",
              " 778: 'give',\n",
              " 779: 'longer',\n",
              " 780: 'lies',\n",
              " 781: 'et',\n",
              " 782: 'son',\n",
              " 783: 'betrayed',\n",
              " 784: 'men',\n",
              " 785: 'saying',\n",
              " 786: 'lied',\n",
              " 787: 'journey',\n",
              " 788: 'long',\n",
              " 789: 'follow',\n",
              " 790: 'sometimes',\n",
              " 791: 'frightening',\n",
              " 792: 'believed',\n",
              " 793: 'suppose',\n",
              " 794: 'both',\n",
              " 795: 'worlds',\n",
              " 796: 'today',\n",
              " 797: 'ignore',\n",
              " 798: 'council',\n",
              " 799: 'thoughts',\n",
              " 800: 'without',\n",
              " 801: 'nor',\n",
              " 802: 'return',\n",
              " 803: 'after',\n",
              " 804: 'indian',\n",
              " 805: 'vice',\n",
              " 806: 'sin',\n",
              " 807: 'wash',\n",
              " 808: 'blood',\n",
              " 809: 'war',\n",
              " 810: 'fine',\n",
              " 811: 'ten',\n",
              " 812: 'raise',\n",
              " 813: 'wheel',\n",
              " 814: 'due',\n",
              " 815: 'captain',\n",
              " 816: 'sir',\n",
              " 817: 'wonder',\n",
              " 818: 'sight',\n",
              " 819: 'days',\n",
              " 820: 'surely',\n",
              " 821: 'quadrant',\n",
              " 822: 'read',\n",
              " 823: 'drawing',\n",
              " 824: 'cheated',\n",
              " 825: 'past',\n",
              " 826: 'mad',\n",
              " 827: 'hopes',\n",
              " 828: 'alive',\n",
              " 829: 'ships',\n",
              " 830: 'turning',\n",
              " 831: 'half',\n",
              " 832: 'water',\n",
              " 833: 'nearly',\n",
              " 834: 'jesus',\n",
              " 835: 'maria',\n",
              " 836: 'listened',\n",
              " 837: 'bloody',\n",
              " 838: 'forward',\n",
              " 839: 'granted',\n",
              " 840: 'into',\n",
              " 841: 'every',\n",
              " 842: 'ship',\n",
              " 843: 'returns',\n",
              " 844: 'cargo',\n",
              " 845: 'dying',\n",
              " 846: 'proves',\n",
              " 847: 'expensive',\n",
              " 848: 'majesty',\n",
              " 849: 'worse',\n",
              " 850: 'ordered',\n",
              " 851: 'five',\n",
              " 852: 'members',\n",
              " 853: 'suggest',\n",
              " 854: 'replaced',\n",
              " 855: 'attorney',\n",
              " 856: 'lawyer',\n",
              " 857: 'shit',\n",
              " 858: 'coming',\n",
              " 859: 'brought',\n",
              " 860: 'mail',\n",
              " 861: 'women',\n",
              " 862: 'mostly',\n",
              " 863: 'wants',\n",
              " 864: 'clothes',\n",
              " 865: 'sent',\n",
              " 866: 'check',\n",
              " 867: 'cigarettes',\n",
              " 868: 'delusions',\n",
              " 869: 'paranoia',\n",
              " 870: 'recently',\n",
              " 871: 'disappeared',\n",
              " 872: 'everywhere',\n",
              " 873: 'maybe',\n",
              " 874: 'worry',\n",
              " 875: 'movie',\n",
              " 876: 'rights',\n",
              " 877: 'book',\n",
              " 878: 'cut',\n",
              " 879: 'fair',\n",
              " 880: 'background',\n",
              " 881: 'young',\n",
              " 882: 'parents',\n",
              " 883: 'killed',\n",
              " 884: 'mother',\n",
              " 885: 'giving',\n",
              " 886: 'birth',\n",
              " 887: 'fucking',\n",
              " 888: 'doctor',\n",
              " 889: 'czech',\n",
              " 890: 'gave',\n",
              " 891: 'drugs',\n",
              " 892: 'made',\n",
              " 893: \"mother's\",\n",
              " 894: 'famous',\n",
              " 895: 'caught',\n",
              " 896: 'mental',\n",
              " 897: 'hospital',\n",
              " 898: 'alright',\n",
              " 899: 'homicide',\n",
              " 900: 'become',\n",
              " 901: 'custody',\n",
              " 902: 'police',\n",
              " 903: 'department',\n",
              " 904: 'cooperate',\n",
              " 905: 'da',\n",
              " 906: 'place',\n",
              " 907: 'quarters',\n",
              " 908: 'practicing',\n",
              " 909: 'putting',\n",
              " 910: 'fires',\n",
              " 911: 'station',\n",
              " 912: 'empty',\n",
              " 913: 'prostitute',\n",
              " 914: 'whore',\n",
              " 915: 'gay',\n",
              " 916: 'jealous',\n",
              " 917: 'process',\n",
              " 918: 'eddie',\n",
              " 919: 'recommended',\n",
              " 920: 'glad',\n",
              " 921: 'met',\n",
              " 922: 'coffee',\n",
              " 923: 'kitchen',\n",
              " 924: 'problems',\n",
              " 925: 'drag',\n",
              " 926: 'partner',\n",
              " 927: 'friend',\n",
              " 928: 'mine',\n",
              " 929: 'shower',\n",
              " 930: 'broken',\n",
              " 931: 'theirs',\n",
              " 932: 'whether',\n",
              " 933: 'comes',\n",
              " 934: 'arrest',\n",
              " 935: 'cop',\n",
              " 936: 'town',\n",
              " 937: 'south',\n",
              " 938: 'civilian',\n",
              " 939: 'happening',\n",
              " 940: 'truth',\n",
              " 941: 'decision',\n",
              " 942: 'deputy',\n",
              " 943: 'press',\n",
              " 944: 'conference',\n",
              " 945: 'yours',\n",
              " 946: 'ready',\n",
              " 947: 'cause',\n",
              " 948: 'origin',\n",
              " 949: 'reporter',\n",
              " 950: 'ladder',\n",
              " 951: '20',\n",
              " 952: 'rock',\n",
              " 953: 'training',\n",
              " 954: 'stopped',\n",
              " 955: 'cleaned',\n",
              " 956: 'tried',\n",
              " 957: 'mug',\n",
              " 958: 'forgot',\n",
              " 959: 'tree',\n",
              " 960: 'screwed',\n",
              " 961: 'innocent',\n",
              " 962: 'career',\n",
              " 963: 'probably',\n",
              " 964: 'image',\n",
              " 965: 'overtime',\n",
              " 966: \"somethin'\",\n",
              " 967: 'alert',\n",
              " 968: 'media',\n",
              " 969: 'deposition',\n",
              " 970: 'finished',\n",
              " 971: 'gotta',\n",
              " 972: 'vodka',\n",
              " 973: \"takin'\",\n",
              " 974: 'bath',\n",
              " 975: 'id',\n",
              " 976: 'street',\n",
              " 977: 'hit',\n",
              " 978: 'gun',\n",
              " 979: 'motherfucker',\n",
              " 980: 'whole',\n",
              " 981: 'few',\n",
              " 982: 'questions',\n",
              " 983: \"nothin'\",\n",
              " 984: 'king',\n",
              " 985: 'edward',\n",
              " 986: 'hotel',\n",
              " 987: 'sudden',\n",
              " 988: 'cousin',\n",
              " 989: 'works',\n",
              " 990: 'describe',\n",
              " 991: 'scary',\n",
              " 992: 'second',\n",
              " 993: 'build',\n",
              " 994: 'turn',\n",
              " 995: 'quickly',\n",
              " 996: 'crime',\n",
              " 997: 'scene',\n",
              " 998: 'news',\n",
              " 999: 'nah',\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBQ0VY1xJiQM"
      },
      "source": [
        "for i in range(len(clean_answers)):\n",
        "    clean_answers[i] += ' <EOS>'"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xU2sZPTtKQAz"
      },
      "source": [
        "Mapping Questions and Answers into Integers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5KNNSO2KPLR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4ef16b7-a91e-4369-fe05-3f1e36dba5d1"
      },
      "source": [
        "questions_into_int = []\n",
        "\n",
        "for question in clean_questions:\n",
        "    ints= []\n",
        "    for word in question.split():\n",
        "        if word not in questionswords2int:\n",
        "            ints.append(questionswords2int['<OUT>'])\n",
        "        else:\n",
        "            ints.append(questionswords2int[word])\n",
        "    questions_into_int.append(ints)\n",
        "questions_into_int[0]"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 6997,\n",
              " 6997,\n",
              " 5,\n",
              " 6,\n",
              " 6997,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 10,\n",
              " 6997,\n",
              " 11,\n",
              " 12,\n",
              " 13,\n",
              " 14,\n",
              " 15,\n",
              " 6997,\n",
              " 16]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_93SAmWKeJq"
      },
      "source": [
        "answers_into_int = []\n",
        "\n",
        "for answer in clean_answers:\n",
        "    ints = []\n",
        "    for word in answer.split():\n",
        "        if word not in answerswords2int:\n",
        "            ints.append(answerswords2int['<OUT>'])\n",
        "        else:\n",
        "            ints.append(answerswords2int[word])\n",
        "    answers_into_int.append(ints)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIViODkyMuvZ"
      },
      "source": [
        "Sorting Questions and Answers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1VTRJ08M0ai"
      },
      "source": [
        "sorted_clean_questions = []\n",
        "sorted_clean_answers = []\n",
        "\n",
        "for length in range(1, 25+1):\n",
        "    for i in enumerate(questions_into_int):\n",
        "        if len(i[1]) == length:\n",
        "            sorted_clean_questions.append(questions_into_int[i[0]])\n",
        "            sorted_clean_answers.append(answers_into_int[i[0]])"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yku6m02MNzDM"
      },
      "source": [
        ""
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cSJy8gpRy_P"
      },
      "source": [
        "# Building the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lspec8IR9KZ"
      },
      "source": [
        "Defining the Model Inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I30CKGufR3A9"
      },
      "source": [
        "def model_inputs():\n",
        "    inputs = tf.placeholder(tf.int32, [None, None], name = 'input')\n",
        "    targets = tf.placeholder(tf.int32, [None, None], name = 'target')\n",
        "    lr = tf.placeholder(tf.float32, name = 'learning_rate')\n",
        "    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
        "    return inputs, targets, lr, keep_prob"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVXajL3rUxMg"
      },
      "source": [
        "Pre-processing the targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF0L1i6dSAMm"
      },
      "source": [
        "def preprocess_targets(targets, word2int, batch_size):\n",
        "    left_side = tf.fill([batch_size, 1], word2int['<SOS>'])\n",
        "    right_side = tf.strided_slice(targets, [0,0], [batch_size, -1], [1,1])\n",
        "    preprocessed_targets = tf.concat([left_side, right_side], 1)\n",
        "    return preprocessed_targets"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsWNEoU-VEMY"
      },
      "source": [
        "Encoder RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_z1gu4PU9nY"
      },
      "source": [
        "def encoder_rnn(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length):\n",
        "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
        "    lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
        "    encoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
        "    encoder_output, encoder_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = encoder_cell,\n",
        "                                                       cell_bw = encoder_cell,\n",
        "                                                       sequence_length = sequence_length,\n",
        "                                                       inputs = rnn_inputs,\n",
        "                                                       dtype = tf.float32)\n",
        "    return encoder_state"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQylgOXHVJfD"
      },
      "source": [
        "Decoding the Training Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUvZj8FaVG_8"
      },
      "source": [
        "def decode_training_set(encoder_state, decoder_cell, decoder_embedded_input, sequence_length, decoding_scope, output_function, keep_prob, batch_size):\n",
        "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
        "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states,\n",
        "                                                                                                                                    attention_option = 'bahdanau',\n",
        "                                                                                                                                    num_units = decoder_cell.output_size)\n",
        "    training_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0],\n",
        "                                                                              attention_keys,\n",
        "                                                                              attention_values,\n",
        "                                                                              attention_score_function,\n",
        "                                                                              attention_construct_function,\n",
        "                                                                              name = 'attn_dec_train')\n",
        "    decoder_output, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
        "                                                                                                              training_decoder_function,\n",
        "                                                                                                              decoder_embedded_input,\n",
        "                                                                                                              sequence_length,\n",
        "                                                                                                              scope = decoding_scope)\n",
        "    decoder_output_dropout = tf.nn.dropout(decoder_output, keep_prob)\n",
        "    return output_function(decoder_output_dropout)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVRiu2uAVPVQ"
      },
      "source": [
        "Decode the Testing set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMnyMAZxVOvL"
      },
      "source": [
        "def decode_test_set(encoder_state, decoder_cell, decoder_embeddings_matrix, sos_id, eos_id, maximum_length, num_words, decoding_scope, output_function, keep_prob, batch_size):\n",
        "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
        "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n",
        "    test_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_inference(output_function,\n",
        "                                                                              encoder_state[0],\n",
        "                                                                              attention_keys,\n",
        "                                                                              attention_values,\n",
        "                                                                              attention_score_function,\n",
        "                                                                              attention_construct_function,\n",
        "                                                                              decoder_embeddings_matrix,\n",
        "                                                                              sos_id,\n",
        "                                                                              eos_id,\n",
        "                                                                              maximum_length,\n",
        "                                                                              num_words,\n",
        "                                                                              name = \"attn_dec_inf\")\n",
        "    test_predictions, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
        "                                                                                                                test_decoder_function,\n",
        "                                                                                                                scope = decoding_scope)\n",
        "    return test_predictions"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrweZtn3VWN4"
      },
      "source": [
        "Decoder RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwTHXp1FVVk4"
      },
      "source": [
        "def decoder_rnn(decoder_embedded_input, decoder_embeddings_matrix, encoder_state, num_words, sequence_length, rnn_size, num_layers, word2int, keep_prob, batch_size):\n",
        "    with tf.variable_scope(\"decoding\") as decoding_scope:\n",
        "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
        "        lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
        "        decoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
        "        weights = tf.truncated_normal_initializer(stddev = 0.1)\n",
        "        biases = tf.zeros_initializer()\n",
        "        output_function = lambda x: tf.contrib.layers.fully_connected(x,\n",
        "                                                                      num_words,\n",
        "                                                                      None,\n",
        "                                                                      scope = decoding_scope,\n",
        "                                                                      weights_initializer = weights,\n",
        "                                                                      biases_initializer = biases)\n",
        "        training_predictions = decode_training_set(encoder_state,\n",
        "                                                   decoder_cell,\n",
        "                                                   decoder_embedded_input,\n",
        "                                                   sequence_length,\n",
        "                                                   decoding_scope,\n",
        "                                                   output_function,\n",
        "                                                   keep_prob,\n",
        "                                                   batch_size)\n",
        "        decoding_scope.reuse_variables()\n",
        "        test_predictions = decode_test_set(encoder_state,\n",
        "                                           decoder_cell,\n",
        "                                           decoder_embeddings_matrix,\n",
        "                                           word2int['<SOS>'],\n",
        "                                           word2int['<EOS>'],\n",
        "                                           sequence_length - 1,\n",
        "                                           num_words,\n",
        "                                           decoding_scope,\n",
        "                                           output_function,\n",
        "                                           keep_prob,\n",
        "                                           batch_size)\n",
        "    return training_predictions, test_predictions"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvUmvrurVeRE"
      },
      "source": [
        "Sequence-to-Sequence Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCZ3d6uSVdpI"
      },
      "source": [
        "def seq2seq_model(inputs,targets, keep_prob, batch_size, sequence_length, answers_num_words, questions_num_words,\n",
        "                  encoder_embedding_size, decoder_embedding_size, rnn_size, num_layers, questionswords2int):\n",
        "    encoder_embedded_input = tf.contrib.layers.embed_sequence(inputs,\n",
        "                                                              answers_num_words+1,\n",
        "                                                              encoder_embedding_size,\n",
        "                                                              initializer = tf.random_uniform_initializer(0,1))\n",
        "    encoder_state = encoder_rnn(encoder_embedded_input, rnn_size, num_layers, keep_prob, sequence_length)\n",
        "    preprocessed_targets = preprocess_targets(targets, questionswords2int, batch_size)\n",
        "    decoder_embedded_matrix = tf.Variable(tf.random_uniform([questions_num_words+1, decoder_embedding_size], 0, 1))\n",
        "    decoder_embedded_input = tf.nn.embedding_lookup(decoder_embedded_matrix, preprocessed_targets)\n",
        "    training_predictions, test_predictions = decoder_rnn(decoder_embedded_input, decoder_embedded_matrix,\n",
        "                                                         encoder_state,\n",
        "                                                         questions_num_words,\n",
        "                                                         sequence_length,\n",
        "                                                         rnn_size,\n",
        "                                                         num_layers,\n",
        "                                                         questionswords2int,\n",
        "                                                         keep_prob,\n",
        "                                                         batch_size)\n",
        "    return training_predictions, test_predictions"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pup75Rwo5Rsh"
      },
      "source": [
        "# Training the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJa4-5N75amf"
      },
      "source": [
        "Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FD189CVaVl5g"
      },
      "source": [
        "epochs = 100\n",
        "batch_size = 32\n",
        "rnn_size = 1024\n",
        "num_layers = 3\n",
        "encoding_embedding_size = 1024\n",
        "decoding_embedding_size = 1024\n",
        "learning_rate = 0.001\n",
        "learning_rate_decay = 0.9\n",
        "min_learning_rate = 0.0001\n",
        "keep_probability = 0.5"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chexpbLP5mY8",
        "outputId": "692bdb58-5332-458f-c144-c04c102a39f4"
      },
      "source": [
        "!pip install tensorflow==1.0.0"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/58/b71480f9ec9d08d581d672a81b15ab5fec36a5fcda2093558a23614d8468/tensorflow-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (44.5MB)\n",
            "\u001b[K     || 44.5MB 97kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (3.12.4)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (1.18.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (0.35.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.1.0->tensorflow==1.0.0) (50.3.2)\n",
            "Installing collected packages: tensorflow\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed tensorflow-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "EkbPyqqv55FY",
        "outputId": "8227c36a-4b2f-43bc-dda9-71fe606bfd78"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tf.__version__"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:474: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:475: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.0.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWgjWuVu5eAX"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "session = tf.InteractiveSession()\n",
        "\n",
        "\n",
        "# Loading the model inputs\n",
        "inputs, targets, lr, keep_prob = model_inputs()\n",
        "\n",
        "\n",
        "# Setting the sequence length\n",
        "sequence_length = tf.placeholder_with_default(25, None, name = 'sequence_length')\n",
        "\n",
        "\n",
        "# Getting the shape of the inputs tensor\n",
        "input_shape = tf.shape(inputs)\n",
        "\n",
        "\n",
        "# Getting the training and test predictions\n",
        "training_predictions, test_predictions = seq2seq_model(tf.reverse(inputs, [-1]),\n",
        "                                                       targets,\n",
        "                                                       keep_prob,\n",
        "                                                       batch_size,\n",
        "                                                       sequence_length,\n",
        "                                                       len(answerswords2int),\n",
        "                                                       len(questionswords2int),\n",
        "                                                       encoding_embedding_size,\n",
        "                                                       decoding_embedding_size,\n",
        "                                                       rnn_size,\n",
        "                                                       num_layers,\n",
        "                                                       questionswords2int)"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxnt7dx_6PzY"
      },
      "source": [
        "Setting up the Loss Error, the Optimizer and Gradient Clipping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzKoWF476L4t"
      },
      "source": [
        "with tf.name_scope(\"optimization\"):\n",
        "    loss_error = tf.contrib.seq2seq.sequence_loss(training_predictions,\n",
        "                                                  targets,\n",
        "                                                  tf.ones([input_shape[0], sequence_length]))\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "    gradients = optimizer.compute_gradients(loss_error)\n",
        "    clipped_gradients = [(tf.clip_by_value(grad_tensor, -5., 5.), grad_variable) for grad_tensor, grad_variable in gradients if grad_tensor is not None]\n",
        "    optimizer_gradient_clipping = optimizer.apply_gradients(clipped_gradients)"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxO7obJh6V8c"
      },
      "source": [
        "Padding and Splitting the data into Questions and Answers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpGIugfI6RDa"
      },
      "source": [
        "def apply_padding(batch_of_sequences, word2int):\n",
        "    max_sequence_length = max([len(sequence) for sequence in batch_of_sequences])\n",
        "    return [sequence + [word2int['<PAD>']] * (max_sequence_length - len(sequence)) for sequence in batch_of_sequences]\n",
        "\n",
        "def split_into_batches(questions, answers, batch_size):\n",
        "    for batch_index in range(0, len(questions) // batch_size):\n",
        "        start_index = batch_index * batch_size\n",
        "        questions_in_batch = questions[start_index : start_index + batch_size]\n",
        "        answers_in_batch = answers[start_index : start_index + batch_size]\n",
        "        padded_questions_in_batch = np.array(apply_padding(questions_in_batch, questionswords2int))\n",
        "        padded_answers_in_batch = np.array(apply_padding(answers_in_batch, answerswords2int))\n",
        "        yield padded_questions_in_batch, padded_answers_in_batch"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvDy6QTH6jED"
      },
      "source": [
        "Splitting batches into Train and Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylVpCDJM6dZV"
      },
      "source": [
        "training_validation_split = int(len(sorted_clean_questions) * 0.15)\n",
        "training_questions = sorted_clean_questions[training_validation_split:]\n",
        "training_answers = sorted_clean_answers[training_validation_split:]\n",
        "validation_questions = sorted_clean_questions[:training_validation_split]\n",
        "validation_answers = sorted_clean_answers[:training_validation_split]"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EoVZpmN6tsM"
      },
      "source": [
        "Training = ~5 DAYS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORgw0czZ6rSD",
        "outputId": "f3f8830f-85c5-4538-edd2-703d9ffe9d40"
      },
      "source": [
        "# 3.11 Training\n",
        "batch_index_check_training_loss = 100\n",
        "batch_index_check_validation_loss = ((len(training_questions)) // batch_size // 2) - 1\n",
        "total_training_loss_error = 0\n",
        "list_validation_loss_error = []\n",
        "early_stopping_check = 0\n",
        "early_stopping_stop = 100\n",
        "checkpoint = \"chatbot_weights.ckpt\" # For Windows, replace this line of code by: checkpoint = \"./chatbot_weights.ckpt\"\n",
        "session.run(tf.global_variables_initializer())\n",
        "for epoch in range(1, epochs + 1):\n",
        "    for batch_index, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(training_questions, training_answers, batch_size)):\n",
        "        starting_time = time.time()\n",
        "        _, batch_training_loss_error = session.run([optimizer_gradient_clipping, loss_error], {inputs: padded_questions_in_batch,\n",
        "                                                                                               targets: padded_answers_in_batch,\n",
        "                                                                                               lr: learning_rate,\n",
        "                                                                                               sequence_length: padded_answers_in_batch.shape[1],\n",
        "                                                                                               keep_prob: keep_probability})\n",
        "        total_training_loss_error += batch_training_loss_error\n",
        "        ending_time = time.time()\n",
        "        batch_time = ending_time - starting_time\n",
        "        if batch_index % batch_index_check_training_loss == 0:\n",
        "            print('Epoch: {:>3}/{}, Batch: {:>4}/{}, Training Loss Error: {:>6.3f}, Training Time on 100 Batches: {:d} seconds'.format(epoch,\n",
        "                                                                                                                                       epochs,\n",
        "                                                                                                                                       batch_index,\n",
        "                                                                                                                                       len(training_questions) // batch_size,\n",
        "                                                                                                                                       total_training_loss_error / batch_index_check_training_loss,\n",
        "                                                                                                                                       int(batch_time * batch_index_check_training_loss)))\n",
        "            total_training_loss_error = 0\n",
        "        if batch_index % batch_index_check_validation_loss == 0 and batch_index > 0:\n",
        "            total_validation_loss_error = 0\n",
        "            starting_time = time.time()\n",
        "            for batch_index_validation, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(validation_questions, validation_answers, batch_size)):\n",
        "                batch_validation_loss_error = session.run(loss_error, {inputs: padded_questions_in_batch,\n",
        "                                                                       targets: padded_answers_in_batch,\n",
        "                                                                       lr: learning_rate,\n",
        "                                                                       sequence_length: padded_answers_in_batch.shape[1],\n",
        "                                                                       keep_prob: 1})\n",
        "                total_validation_loss_error += batch_validation_loss_error\n",
        "            ending_time = time.time()\n",
        "            batch_time = ending_time - starting_time\n",
        "            average_validation_loss_error = total_validation_loss_error / (len(validation_questions) / batch_size)\n",
        "            print('Validation Loss Error: {:>6.3f}, Batch Validation Time: {:d} seconds'.format(average_validation_loss_error, int(batch_time)))\n",
        "            learning_rate *= learning_rate_decay\n",
        "            if learning_rate < min_learning_rate:\n",
        "                learning_rate = min_learning_rate\n",
        "            list_validation_loss_error.append(average_validation_loss_error)\n",
        "            if average_validation_loss_error <= min(list_validation_loss_error):\n",
        "                print('I speak better now!!')\n",
        "                early_stopping_check = 0\n",
        "                saver = tf.train.Saver()\n",
        "                saver.save(session, checkpoint)\n",
        "            else:\n",
        "                print(\"Sorry I do not speak better, I need to practice more.\")\n",
        "                early_stopping_check += 1\n",
        "                if early_stopping_check == early_stopping_stop:\n",
        "                    break\n",
        "    if early_stopping_check == early_stopping_stop:\n",
        "        print(\"My apologies, I cannot speak better anymore. This is the best I can do.\")\n",
        "        break\n",
        "print(\"Game Over\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:   1/100, Batch:    0/4120, Training Loss Error:  0.089, Training Time on 100 Batches: 1586 seconds\n",
            "Epoch:   1/100, Batch:  100/4120, Training Loss Error:  2.944, Training Time on 100 Batches: 1679 seconds\n",
            "Epoch:   1/100, Batch:  200/4120, Training Loss Error:  2.358, Training Time on 100 Batches: 1365 seconds\n",
            "Epoch:   1/100, Batch:  300/4120, Training Loss Error:  2.284, Training Time on 100 Batches: 1642 seconds\n",
            "Epoch:   1/100, Batch:  400/4120, Training Loss Error:  2.201, Training Time on 100 Batches: 1558 seconds\n",
            "Epoch:   1/100, Batch:  500/4120, Training Loss Error:  2.221, Training Time on 100 Batches: 1692 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zAcvyI86yBS"
      },
      "source": [
        "# Chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbTJZDTa604e"
      },
      "source": [
        "Testing the seq2seq model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2l-u2DP60Ww"
      },
      "source": [
        "checkpoint = \"./chatbot_weights.ckpt\"\n",
        "session = tf.InteractiveSession()\n",
        "session.run(tf.global_variables_initializer())\n",
        "saver = tf.train.Saver()\n",
        "saver.restore(session, checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwWMa3wJ7A0s"
      },
      "source": [
        "Converting Strings to integers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3eFgkfd7BOF"
      },
      "source": [
        "def convert_string2int(question, word2int):\n",
        "    question = clean_text(question)\n",
        "    return [word2int.get(word, word2int['<OUT>']) for word in question.split()]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKXxh8aZ7Fo6"
      },
      "source": [
        "Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4Cj-UsR7FCT",
        "outputId": "4d4d0511-ebb7-4a3d-ae02-0e00355e5e2f"
      },
      "source": [
        "while(True):\n",
        "    question = input(\"You: \")\n",
        "    if question == \"Goodbye\".lower() or \"Bye\".lower() or \"Quit\".lower() or \"Exit\".lower():\n",
        "      break\n",
        "    question = convert_string2int(question, questionswords2int)\n",
        "    question = question + [questionswords2int['<PAD>']] * (20 - len(question))\n",
        "    fake_batch = np.zeros((batch_size, 20))\n",
        "    fake_batch[0] = question\n",
        "    predicted_answer = session.run(test_predictions, {inputs: fake_batch,\n",
        "                                                      keep_prob: 0.5})[0]\n",
        "    answer = ''\n",
        "    for i in np.argmax(predicted_answer, 1):\n",
        "        if answersints2word[i] == 'i':\n",
        "            token = ' I'\n",
        "        elif answersints2word[i] == '<EOS>':\n",
        "            token = '.'\n",
        "        elif answersints2word[i] == '<OUT>':\n",
        "            token = 'out'\n",
        "        else:\n",
        "            token = ' ' + answersints2word[i]\n",
        "        answer += token\n",
        "        if token == '.':\n",
        "            break\n",
        "    print('ChatBot: ' + answer)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You: hello\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKomqW-7OSWJ"
      },
      "source": [
        "question1 = \"how are you\""
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVXvuGlJOcEv",
        "outputId": "33fdd01e-fcfa-4df3-8b5e-e3ab7128ed3b"
      },
      "source": [
        "question = convert_string2int(question1, questionswords2int)\n",
        "question"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[88, 7, 27]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgEdDLmEOjHe"
      },
      "source": [
        "question = question + [questionswords2int['<PAD>']] * (20 - len(question))\n",
        "question"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8U8dEUrK6Ve"
      },
      "source": [
        "fake_batch = np.zeros((32, 20))\n",
        "fake_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIgZPBRYOn7N"
      },
      "source": [
        "fake_batch[0] = question\n",
        "fake_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZmEjxYjOPIh"
      },
      "source": [
        "predicted_answer = session.run(test_predictions, {inputs: fake_batch,\n",
        "                                                      keep_prob: 0.5})[0]"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjAazWLSqtoU",
        "outputId": "bea83ef3-7aaf-432c-822c-36558561ec10"
      },
      "source": [
        "predicted_answer"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.0604356 ,  0.09897618, -0.23355635, ...,  0.00681708,\n",
              "        -0.25178632, -0.10273945],\n",
              "       [ 0.08545911,  0.12463835, -0.11570345, ..., -0.03829306,\n",
              "        -0.1725347 , -0.05692882],\n",
              "       [ 0.06660751,  0.08435404, -0.04960331, ..., -0.11056082,\n",
              "        -0.12095065,  0.00257137],\n",
              "       ...,\n",
              "       [-0.08979229, -0.03995859, -0.00238437, ...,  0.14559256,\n",
              "        -0.01611924, -0.10515501],\n",
              "       [-0.0358707 , -0.00271157, -0.02162158, ...,  0.10653662,\n",
              "         0.05966359, -0.09595382],\n",
              "       [ 0.02271633,  0.00543448, -0.00410741, ...,  0.03106881,\n",
              "         0.0184293 , -0.07154968]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rB2A2XXaqoBU",
        "outputId": "6b443438-d371-4e00-fe2d-f63feb52351a"
      },
      "source": [
        "type(predicted_answer)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bqu4hOJvqw_b",
        "outputId": "f0a778a3-7bfd-497e-e4c5-69a1ccdad031"
      },
      "source": [
        "len(predicted_answer)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4Qb-uxcqygI"
      },
      "source": [
        "answer = ''\n",
        "for i in np.argmax(predicted_answer, 1):\n",
        "  if answersints2word[i] == 'i':\n",
        "    token = ' I'\n",
        "  elif answersints2word[i] == '<EOS>':\n",
        "    token = '.'\n",
        "  elif answersints2word[i] == '<OUT>':\n",
        "    token = 'out'\n",
        "  else:\n",
        "    token = ' ' + answersints2word[i]\n",
        "  answer += token\n",
        "  if token == '.':\n",
        "    break"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "JEl3MCtzrMKZ",
        "outputId": "68d437f3-cb89-455c-ca66-b54f2ca60a39"
      },
      "source": [
        "answer"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' religious hug hug hug hug felson removed felson freaking promises interested interested cared cared cared sports lawyer fingerprints fingerprints herself herself fruit river river river'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_dqpPcVrNcR",
        "outputId": "72163984-e4d0-41e1-ea24-b70218d40023"
      },
      "source": [
        "len(answer)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "186"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqd_7lYzr9ul",
        "outputId": "5ead1a23-3829-43ef-da72-dc02f09bd1e9"
      },
      "source": [
        "type(answersints2word)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3A694Gp2Yth"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}